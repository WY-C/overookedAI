from my_env.Rllib_multi_agent import Rllib_multi_agent

import ray
from ray.rllib.algorithms.ppo import PPOConfig
import gymnasium as gym
import numpy as np

from ray.rllib.algorithms.ppo import PPO
import os
ACTION_MAP = {
    0: (0, -1),   # NORTH
    1: (0, 1),    # SOUTH
    2: (1, 0),    # EAST
    3: (-1, 0),   # WEST
    4: (0, 0),    # STAY
    5: "interact" # INTERACT
}


my_env = Rllib_multi_agent()


from ray.tune.registry import register_env

def env_creator(config):
    return Rllib_multi_agent(config)

register_env("Rllib_multi_agent", env_creator)




ray.init()

def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):
    return "shared_policy"

sample_obs = my_env._get_obs(0)
flattened_shape = sample_obs['agent_1'].flatten().shape

config = (
    PPOConfig()
    .environment(env="Rllib_multi_agent")
    .framework("torch")
    .env_runners(
        # ì´ ê°’ì„ í™˜ê²½ì˜ horizonë³´ë‹¤ ì‘ê±°ë‚˜ ë¹„ìŠ·í•œ ê°’ìœ¼ë¡œ ì„¤ì •í•´ë³´ì„¸ìš”.
        # 'auto'ê°€ ê¸°ë³¸ê°’ì´ì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ë””ë²„ê¹…ì— ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        rollout_fragment_length=400
    )
    .multi_agent(
        policies={
            "shared_policy": (
                None,
                gym.spaces.Box(low=0, high=1, shape=flattened_shape, dtype=np.float32),
                gym.spaces.Discrete(len(ACTION_MAP)),
                {}
            )
        },
        policy_mapping_fn=policy_mapping_fn,
    )
)

def save_checkpoint_with_reward(trainer, reward, results, base_dir="./checkpoints"):
    # rewardë¥¼ ì†Œìˆ˜ì  ë‘˜ì§¸ ìë¦¬ê¹Œì§€ ë¬¸ìì—´ë¡œ ë³€í™˜, ì†Œìˆ˜ì  ëŒ€ì‹  ì–¸ë”ë°”ë¡œ ë³€ê²½(í´ë”ëª… ì•ˆì „í•˜ê²Œ)
    reward_str = f"{reward:.2f}".replace('.', '_')

    # ì €ì¥í•  í´ë” ê²½ë¡œ ìƒì„±
    save_dir = os.path.join(base_dir, f"reward_{reward_str}_rewardshaping")
    save_dir = os.path.abspath(save_dir)
    os.makedirs(save_dir, exist_ok=True)

    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    checkpoint_path = trainer.save(checkpoint_dir=save_dir)
    #print(f"Checkpoint saved at: {checkpoint_path}")
      # --- ì›í•˜ëŠ” í•µì‹¬ ì •ë³´ë§Œ ì„ íƒí•˜ì—¬ ì¶œë ¥ ---
    print("\n--- ğŸ‰ New Best Model Saved! ---")
    print(f"  Iteration:     {results['training_iteration']}")
    print(f"  Mean Reward:   {reward:.2f}")
    print(f"  Policy Loss:   {results['learners']['shared_policy']['policy_loss']:.4f}")
    print(f"  Value Loss:    {results['learners']['shared_policy']['vf_loss']:.4f}")
    #print(f"  Saved to:      {checkpoint_path}")
    print("---------------------------------\n")
    return checkpoint_path


        

rllib_trainer = PPO(config=config)
best_mean_reward = -1


while True:
    results = rllib_trainer.train()
    iteration = results["training_iteration"]
    current_mean_reward = results["env_runners"]["episode_return_mean"]

    if current_mean_reward > best_mean_reward:
        best_mean_reward = current_mean_reward
        
        # ëª¨ë¸ì„ ì €ì¥í•˜ê³ , ì €ì¥ ê²°ê³¼ ê°ì²´ë¥¼ ë°›ìŠµë‹ˆë‹¤.
        save_checkpoint_with_reward(rllib_trainer, current_mean_reward, results)
        # ì €ì¥ ê²°ê³¼ì—ì„œ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
       


    else:
        # ìµœê³  ê¸°ë¡ì„ ê°±ì‹ í•˜ì§€ ëª»í•œ ê²½ìš°, í˜„ì¬ ìƒíƒœë§Œ ê°„ë‹¨íˆ ì¶œë ¥
        print(
            f"Iter: {results['training_iteration']:<3} | "
            f"Reward: {current_mean_reward:<7.2f} | "
            f"Best: {best_mean_reward:<7.2f} | "
            f"Timesteps: {results['num_env_steps_sampled_lifetime']}"
        )
